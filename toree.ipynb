{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toree\n",
    "\n",
    "- [RDD Programming Guide](http://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4.6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@9d7d8f7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark.app.id=local-1594481743745\n",
       "spark.app.name=Apache Toree\n",
       "spark.driver.host=node-master\n",
       "spark.driver.memory=512m\n",
       "spark.driver.port=45043\n",
       "spark.eventLog.dir=hdfs://node-master:54310/spark-logs\n",
       "spark.eventLog.enabled=true\n",
       "spark.executor.id=driver\n",
       "spark.executor.memory=512m\n",
       "spark.history.fs.logDirectory=hdfs://node-master:54310/spark-logs\n",
       "spark.history.fs.update.interval=10s\n",
       "spark.history.provider=org.apache.spark.deploy.history.FsHistoryProvider\n",
       "spark.history.ui.port=18080\n",
       "spark.jars=file:/home/hadoop/.local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar\n",
       "spark.master=local[4]\n",
       "spark.repl.class.outputDir=/tmp/spark-46b17dc2-91fa-489e-97bc-a9fd8a810831/repl-3f5264fc-1c5d-46af-a355-dbd8cf57e7f7\n",
       "spark.repl.class.uri=spark://node-master:45043/...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "sc.getConf.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.driver.host,node-master)\n",
      "(spark.history.fs.logDirectory,hdfs://node-master:54310/spark-logs)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.driver.port,45043)\n",
      "(spark.repl.class.uri,spark://node-master:45043/classes)\n",
      "(spark.jars,file:/home/hadoop/.local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar)\n",
      "(spark.history.fs.update.interval,10s)\n",
      "(spark.repl.class.outputDir,/tmp/spark-46b17dc2-91fa-489e-97bc-a9fd8a810831/repl-3f5264fc-1c5d-46af-a355-dbd8cf57e7f7)\n",
      "(spark.app.name,Apache Toree)\n",
      "(spark.driver.memory,512m)\n",
      "(spark.executor.id,driver)\n",
      "(spark.yarn.am.memory,512m)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.master,local[4])\n",
      "(spark.yarn.archive,hdfs:///spark-libs/spark-libs.jar)\n",
      "(spark.yarn.preserve.staging.files,true)\n",
      "(spark.history.provider,org.apache.spark.deploy.history.FsHistoryProvider)\n",
      "(spark.executor.memory,512m)\n",
      "(spark.eventLog.dir,hdfs://node-master:54310/spark-logs)\n",
      "(spark.history.ui.port,18080)\n",
      "(spark.app.id,local-1594481743745)\n"
     ]
    }
   ],
   "source": [
    "spark.sqlContext.getAllConfs.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark://node-master:45043/jars/toree-assembly-0.3.0-incubating.jar\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.listJars.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: String = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "System.setProperty(\"hive.metastore.uris\", \"thrift://node-master:9083\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sqlContext.setConf(\"hive.metastore.uris\", \"thrift://node-master:9083\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.util.NoSuchElementException\n",
       "Message: spark.sql.hive.metastore.version\n",
       "StackTrace:   at org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:2042)\n",
       "  at org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:2042)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:2042)\n",
       "  at org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:74)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.hive.metastore.version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.util.NoSuchElementException\n",
       "Message: spark.sql.hive.metastore.version\n",
       "StackTrace:   at org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:2042)\n",
       "  at org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:2042)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:2042)\n",
       "  at org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:74)\n",
       "  at org.apache.spark.sql.SQLContext.getConf(SQLContext.scala:135)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sqlContext.getConf(\"spark.sql.hive.metastore.version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [color: string, id: bigint ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[color: string, id: bigint ... 4 more fields]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"/user/hadoop/testing/example.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+---+------+-------------+\n",
      "|color| id|lat|lon|number|     physical|\n",
      "+-----+---+---+---+------+-------------+\n",
      "|  red|  1|0.0|0.0|   123|[5'11, 127.5]|\n",
      "| blue|  2|1.0|1.0|   456|[5'11, 150.0]|\n",
      "|green|  3|4.4|3.3|   789| [6'2, 200.4]|\n",
      "+-----+---+---+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- number: long (nullable = true)\n",
      " |-- physical: struct (nullable = true)\n",
      " |    |-- height: string (nullable = true)\n",
      " |    |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:27: error: value parallelize is not a member of org.apache.spark.sql.SparkSession\n",
       "       val rdd = spark.parallelize(1 to 100, 10)\n",
       "                       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = spark.parallelize(1 to 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd = ParallelCollectionRDD[0] at parallelize at <console>:27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at <console>:27"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at <console>:27"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input = hdfs:///user/hadoop/testing/input.txt MapPartitionsRDD[2] at textFile at <console>:27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/hadoop/testing/input.txt MapPartitionsRDD[2] at textFile at <console>:27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = sc.textFile(\"hdfs:///user/hadoop/testing/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenized = MapPartitionsRDD[4] at filter at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[4] at filter at <console>:29"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenized = input\n",
    "  .map(line => line.split(\" \"))\n",
    "  .filter(words => words.size > 0) // remove empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counts = ShuffledRDD[6] at reduceByKey at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[6] at reduceByKey at <console>:29"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val counts = tokenized  // frequency of log levels\n",
    "  .map(words => (words(0), 1))\n",
    "  .reduceByKey((a,b) => a + b, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((INFO,2), (\"\",1), (WARN,1))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2) ShuffledRDD[6] at reduceByKey at <console>:29 []\n",
       " +-(2) MapPartitionsRDD[5] at map at <console>:28 []\n",
       "    |  MapPartitionsRDD[4] at filter at <console>:29 []\n",
       "    |  MapPartitionsRDD[3] at map at <console>:28 []\n",
       "    |  hdfs:///user/hadoop/testing/input.txt MapPartitionsRDD[2] at textFile at <console>:27 []\n",
       "    |  hdfs:///user/hadoop/testing/input.txt HadoopRDD[1] at textFile at <console>:27 []\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "counts.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(INFO,2)\n",
      "(,1)\n",
      "(WARN,1)\n"
     ]
    }
   ],
   "source": [
    "counts.collect.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
