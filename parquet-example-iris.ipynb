{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Parquet with Iris Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_HOME = \"hdfs://node-master:54310/user/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 1.0.5\n",
      "dask 2.30.0\n",
      "pyarrow 3.0.0\n",
      "numpy 1.18.5\n"
     ]
    }
   ],
   "source": [
    "for module in [pd, dask, pa, np]:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted iris_parquet/iris.parq\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r iris_parquet/iris.parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/envs/sgds/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: pyarrow.hdfs.connect is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   class         150 non-null    object \n",
      " 5   date_test     150 non-null    object \n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 21.0 KB\n"
     ]
    }
   ],
   "source": [
    "hdfs = pa.hdfs.connect('node-master', port=54310)\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('sepal_length', pa.float64()),\n",
    "    pa.field('sepal_width', pa.float64()),\n",
    "    pa.field('petal_length', pa.float64()),\n",
    "    pa.field('petal_width', pa.float64()),\n",
    "    pa.field('class', pa.string()),\n",
    "    pa.field('date_test', pa.date32()),\n",
    "])\n",
    "\n",
    "columns = [\n",
    "  'sepal_length',\n",
    "  'sepal_width',\n",
    "  'petal_length',\n",
    "  'petal_width',\n",
    "  'class'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"data/iris.data\", names=columns)\n",
    "df['date_test'] = pd.date_range(\"2020-01-01\", periods=len(df))\n",
    "df['date_test'] = df['date_test'].dt.date\n",
    "\n",
    "df.info(memory_usage='deep')\n",
    "\n",
    "with hdfs.open(\"iris_parquet/iris.parq\", \"wb\") as f:\n",
    "    df.to_parquet(f,\n",
    "                  engine='pyarrow',\n",
    "                  compression='snappy',\n",
    "                  #schema=schema,\n",
    "                  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdelt_parquet\n",
      "gdelt_parquet_2020\n",
      "iris_csv\n",
      "iris_parquet\n",
      "ne_10_states_provinces_parquet\n",
      "ne_110_countries_parquet\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hive -e \"SHOW TABLES;\" 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hive -e 'set parquet.compression=SNAPPY;\n",
    "DROP TABLE IF EXISTS iris_parquet;\n",
    "CREATE EXTERNAL TABLE iris_parquet (\n",
    "    sepal_length DOUBLE,\n",
    "    sepal_width  DOUBLE,\n",
    "    petal_length DOUBLE,\n",
    "    petal_width  DOUBLE,\n",
    "    class        STRING,\n",
    "    date_test    DATE\n",
    ") \n",
    "STORED AS PARQUET\n",
    "LOCATION \"hdfs://node-master:54310/user/hadoop/iris_parquet\";' 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length        \tdouble              \t                    \n",
      "sepal_width         \tdouble              \t                    \n",
      "petal_length        \tdouble              \t                    \n",
      "petal_width         \tdouble              \t                    \n",
      "class               \tstring              \t                    \n",
      "date_test           \tdate                \t                    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hive -e 'DESCRIBE iris_parquet;' 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1\t3.5\t1.4\t0.2\tIris-setosa\t2020-01-01\n",
      "4.9\t3.0\t1.4\t0.2\tIris-setosa\t2020-01-02\n",
      "4.7\t3.2\t1.3\t0.2\tIris-setosa\t2020-01-03\n",
      "4.6\t3.1\t1.5\t0.2\tIris-setosa\t2020-01-04\n",
      "5.0\t3.6\t1.4\t0.2\tIris-setosa\t2020-01-05\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hive -e 'SELECT * FROM iris_parquet LIMIT 5;' 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Parquet\n",
    "\n",
    "- [Best Practices](https://docs.dask.org/en/latest/dataframe-best-practices.html)\n",
    "- [Remote Data](https://docs.dask.org/en/latest/remote-data-services.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r iris_parquet/iris.parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_filepath = HDFS_HOME + \"iris_parquet\"\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('sepal_length', pa.float64()),\n",
    "    pa.field('sepal_width', pa.float64()),\n",
    "    pa.field('petal_length', pa.float64()),\n",
    "    pa.field('petal_width', pa.float64()),\n",
    "    pa.field('class', pa.string())\n",
    "])\n",
    "\n",
    "columns = [\n",
    "  'sepal_length',\n",
    "  'sepal_width',\n",
    "  'petal_length',\n",
    "  'petal_width',\n",
    "  'class' \n",
    "]\n",
    "\n",
    "ddf = dd.read_csv(\"data/iris.data\", names=columns)\n",
    "ddf.to_parquet(dst_filepath,\n",
    "               engine='pyarrow',\n",
    "               schema=schema,\n",
    "               compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgds",
   "language": "python",
   "name": "sgds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
